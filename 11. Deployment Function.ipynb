{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Tokenizer for the selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('./data/tokenizer_lstm_gru.pickle', 'rb') as handle:\n",
    "     tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom scikit-learn estimator for Tokenize and padding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def pad_text(texts, tokenizer,MAX_SEQUENCE_LENGTH):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "class TokenizingAndPaddingText(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, tokenizer): # no *args or **kargs\n",
    "                self.tokenizer = tokenizer  \n",
    "                \n",
    "        def fit(self, X, y=None):\n",
    "             return self  # nothing else to do\n",
    "        \n",
    "        def transform(self, X):\n",
    "            MAX_SEQUENCE_LENGTH = 250\n",
    "            return pad_text(X, self.tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model along with trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, GRU,Dropout, Activation, Input,\\\n",
    " Embedding,Bidirectional,GlobalMaxPool1D,concatenate,Reshape,GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/riteshranjan110/Jigsaw-Unintended-Bias-Toxic-Comment-Classification/blob/master/Jigsaw4.ipynb\n",
    "\n",
    "def LSTM_GRU_model(MAX_SEQUENCE_LENGTH=250,EMBEDDINGS_DIMENSION = 300):\n",
    "    #model_conv = Sequential()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='inputs')\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                        EMBEDDINGS_DIMENSION,\n",
    "                                        #weights=[glove_matrix],  \n",
    "                                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                        trainable=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    layer1 = embedding_layer(sequence_input)\n",
    "    \n",
    "    layer1 = Dropout(0.2)(layer1)\n",
    "    layer1 = Bidirectional(LSTM(120, return_sequences=True))(layer1)\n",
    "    layer1, fowrward_h_state, backward_h_state = Bidirectional(GRU(60, return_sequences= True, return_state=True))(layer1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    h_state = concatenate([fowrward_h_state, backward_h_state])\n",
    "    h_state = Reshape((-1,120))(h_state )\n",
    "\n",
    "    h_avg = GlobalAveragePooling1D()(layer1)\n",
    "    h_max = GlobalMaxPool1D()(layer1)\n",
    "\n",
    "    h_avg = Reshape((-1,120))(h_avg)\n",
    "    h_max = Reshape((-1,120))(h_max)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = concatenate([h_state, h_avg, h_max])\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    #https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    \n",
    "    output = Dense(2, activation='sigmoid')(x)\n",
    "    model = Model(inputs=sequence_input, outputs = output)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"./data/weights_lstm_gru.best.hdf5\"\n",
    "LSTM_GRU_model_1 = LSTM_GRU_model()\n",
    "LSTM_GRU_model_1.load_weights(WEIGHTS_PATH)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sklearn pipeline with 2 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "# Keras Model to sklearn pipeline:https://gist.github.com/MaxHalford/9bfaa8daf8b4bc17a7fb7ba58c880675\n",
    "pipe_1 = pipeline.Pipeline([\n",
    "    ('Tokenize_Padding', TokenizingAndPaddingText(tokenizer)),\n",
    "    ('LSTM_GRU_Model',LSTM_GRU_model_1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_1(text_list):\n",
    "    return list(pipe_1.predict(text_list)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Function 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7097320</td>\n",
       "      <td>[ Integrity means that you pay your debts.]\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097321</td>\n",
       "      <td>This is malfeasance by the Administrator and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7097322</td>\n",
       "      <td>@Rmiller101 - Spoken like a true elitist. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7097323</td>\n",
       "      <td>Paul: Thank you for your kind words.  I do, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7097324</td>\n",
       "      <td>Sorry you missed high school. Eisenhower sent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7097320  [ Integrity means that you pay your debts.]\\n\\...\n",
       "1  7097321  This is malfeasance by the Administrator and t...\n",
       "2  7097322  @Rmiller101 - Spoken like a true elitist. But ...\n",
       "3  7097323  Paul: Thank you for your kind words.  I do, in...\n",
       "4  7097324  Sorry you missed high school. Eisenhower sent ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_embd = pd.read_csv('./data/test.csv', sep=',', quotechar='\"')\n",
    "test_embd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>model_prediction_score</th>\n",
       "      <th>model_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The profoundly stupid have spoken.</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The ignorance and bigotry comes from your post!</td>\n",
       "      <td>0.722706</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Donâ€™t get it do you. As the price of things go...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I bet China would be happy to help Puerto Rico...</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>â€œThis was not an action that was taken lightly...</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text  model_prediction_score  \\\n",
       "15                 The profoundly stupid have spoken.                0.999986   \n",
       "16    The ignorance and bigotry comes from your post!                0.722706   \n",
       "17  Donâ€™t get it do you. As the price of things go...                0.000410   \n",
       "18  I bet China would be happy to help Puerto Rico...                0.001043   \n",
       "19  â€œThis was not an action that was taken lightly...                0.001054   \n",
       "\n",
       "    model_predictions  \n",
       "15               True  \n",
       "16               True  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_1_result=pd.DataFrame({\n",
    "    \"comment_text\":list(test_embd.comment_text[20:40].astype(str)),\n",
    "     \"model_prediction_score\":Function_1(list(test_embd.comment_text[20:40].astype(str)))\n",
    "     \n",
    "})\n",
    "function_1_result[\"model_predictions\"] = np.where(function_1_result[\"model_prediction_score\"] >= 0.5, True, False)\n",
    "function_1_result.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FUNCTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    TOXICITY_COLUMN=\"target\"\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom estimator to convert score columns to Boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df,identity_columns):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "\n",
    "class ConverScoreToBool(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, identity_columns, TOXICITY_COLUMN): # no *args or **kargs\n",
    "                self.identity_columns = identity_columns  \n",
    "                self.TOXICITY_COLUMN = TOXICITY_COLUMN\n",
    "        def fit(self, df, y=None):\n",
    "             return self  # nothing else to do\n",
    "        \n",
    "        def transform(self, df):\n",
    "            return convert_dataframe_to_bool(df,self.identity_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_2(test_df):\n",
    "    identity_columns = [\n",
    "         'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "         'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "    TOXICITY_COLUMN = 'target' \n",
    "    conv_to_bool=ConverScoreToBool(identity_columns,TOXICITY_COLUMN)\n",
    "    test_df=conv_to_bool.transform(test_df) \n",
    "    test_df[\"model_prob\"] = list(pipe_1.predict(test_df.comment_text.astype(str))[:,1])\n",
    "    bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, \"model_prob\", TOXICITY_COLUMN)\n",
    "    evaluation_score  = get_final_metric(bias_metrics_df.fillna(0), calculate_overall_auc(test_df, \"model_prob\"))\n",
    "    \n",
    "    return evaluation_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Evaluation of model we have to collect following columns:\n",
    "<ol>\n",
    "<li> identity_columns = <b>['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', 'white', 'psychiatric_or_mental_illness']</b>\n",
    "Score for toxicity theshold (between 0-1) with each identity columns </li> \n",
    "\n",
    "<li> comment text</li>\n",
    "<li>target: overall threshold for toxicity in comment</li>\n",
    " </ol>\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Train data to test Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849     0.0  Thank you!! This would make my life a lot less...   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "1              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "\n",
       "[2 rows x 45 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embd = pd.read_csv('./data/train.csv', sep=',', quotechar='\"')\n",
    "train_embd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Score of input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24439102564102566"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Function_2(train_embd[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embd.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
