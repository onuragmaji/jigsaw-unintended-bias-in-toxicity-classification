{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Tokenizer for the selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('./data/tokenizer_lstm_gru.pickle', 'rb') as handle:\n",
    "     tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom scikit-learn estimator for Tokenize and padding the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def pad_text(texts, tokenizer,MAX_SEQUENCE_LENGTH):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "class TokenizingAndPaddingText(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, tokenizer): # no *args or **kargs\n",
    "                self.tokenizer = tokenizer  \n",
    "                \n",
    "        def fit(self, X, y=None):\n",
    "             return self  # nothing else to do\n",
    "        \n",
    "        def transform(self, X):\n",
    "            MAX_SEQUENCE_LENGTH = 250\n",
    "            return pad_text(X, self.tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model along with trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, GRU,Dropout, Activation, Input,\\\n",
    " Embedding,Bidirectional,GlobalMaxPool1D,concatenate,Reshape,GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/riteshranjan110/Jigsaw-Unintended-Bias-Toxic-Comment-Classification/blob/master/Jigsaw4.ipynb\n",
    "\n",
    "def LSTM_GRU_model(MAX_SEQUENCE_LENGTH=250,EMBEDDINGS_DIMENSION = 300):\n",
    "    #model_conv = Sequential()\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='inputs')\n",
    "    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                        EMBEDDINGS_DIMENSION,\n",
    "                                        #weights=[glove_matrix],  \n",
    "                                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                        trainable=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    layer1 = embedding_layer(sequence_input)\n",
    "    \n",
    "    layer1 = Dropout(0.2)(layer1)\n",
    "    layer1 = Bidirectional(LSTM(120, return_sequences=True))(layer1)\n",
    "    layer1, fowrward_h_state, backward_h_state = Bidirectional(GRU(60, return_sequences= True, return_state=True))(layer1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    h_state = concatenate([fowrward_h_state, backward_h_state])\n",
    "    h_state = Reshape((-1,120))(h_state )\n",
    "\n",
    "    h_avg = GlobalAveragePooling1D()(layer1)\n",
    "    h_max = GlobalMaxPool1D()(layer1)\n",
    "\n",
    "    h_avg = Reshape((-1,120))(h_avg)\n",
    "    h_max = Reshape((-1,120))(h_max)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = concatenate([h_state, h_avg, h_max])\n",
    "    x = Dense(20, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    #https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    \n",
    "    output = Dense(2, activation='sigmoid')(x)\n",
    "    model = Model(inputs=sequence_input, outputs = output)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = \"./data/weights_lstm_gru.best.hdf5\"\n",
    "LSTM_GRU_model_1 = LSTM_GRU_model()\n",
    "LSTM_GRU_model_1.load_weights(WEIGHTS_PATH)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sklearn pipeline with 2 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "# Keras Model to sklearn pipeline:https://gist.github.com/MaxHalford/9bfaa8daf8b4bc17a7fb7ba58c880675\n",
    "pipe_1 = pipeline.Pipeline([\n",
    "    ('Tokenize_Padding', TokenizingAndPaddingText(tokenizer)),\n",
    "    ('LSTM_GRU_Model',LSTM_GRU_model_1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_1(text_list):\n",
    "    return list(pipe_1.predict(text_list)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Function 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7097320</td>\n",
       "      <td>[ Integrity means that you pay your debts.]\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097321</td>\n",
       "      <td>This is malfeasance by the Administrator and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7097322</td>\n",
       "      <td>@Rmiller101 - Spoken like a true elitist. But ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7097323</td>\n",
       "      <td>Paul: Thank you for your kind words.  I do, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7097324</td>\n",
       "      <td>Sorry you missed high school. Eisenhower sent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text\n",
       "0  7097320  [ Integrity means that you pay your debts.]\\n\\...\n",
       "1  7097321  This is malfeasance by the Administrator and t...\n",
       "2  7097322  @Rmiller101 - Spoken like a true elitist. But ...\n",
       "3  7097323  Paul: Thank you for your kind words.  I do, in...\n",
       "4  7097324  Sorry you missed high school. Eisenhower sent ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_embd = pd.read_csv('./data/test.csv', sep=',', quotechar='\"')\n",
    "test_embd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>model_prediction_score</th>\n",
       "      <th>model_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The profoundly stupid have spoken.</td>\n",
       "      <td>0.999986</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The ignorance and bigotry comes from your post!</td>\n",
       "      <td>0.722706</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Don’t get it do you. As the price of things go...</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I bet China would be happy to help Puerto Rico...</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“This was not an action that was taken lightly...</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         comment_text  model_prediction_score  \\\n",
       "15                 The profoundly stupid have spoken.                0.999986   \n",
       "16    The ignorance and bigotry comes from your post!                0.722706   \n",
       "17  Don’t get it do you. As the price of things go...                0.000410   \n",
       "18  I bet China would be happy to help Puerto Rico...                0.001043   \n",
       "19  “This was not an action that was taken lightly...                0.001054   \n",
       "\n",
       "    model_predictions  \n",
       "15               True  \n",
       "16               True  \n",
       "17              False  \n",
       "18              False  \n",
       "19              False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_1_result=pd.DataFrame({\n",
    "    \"comment_text\":list(test_embd.comment_text[20:40].astype(str)),\n",
    "     \"model_prediction_score\":Function_1(list(test_embd.comment_text[20:40].astype(str)))\n",
    "     \n",
    "})\n",
    "function_1_result[\"model_predictions\"] = np.where(function_1_result[\"model_prediction_score\"] >= 0.5, True, False)\n",
    "function_1_result.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FUNCTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    TOXICITY_COLUMN=\"target\"\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom estimator to convert score columns to Boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting taget and identity columns to booleans\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df,identity_columns):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target'] + identity_columns:\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df\n",
    "\n",
    "\n",
    "class ConverScoreToBool(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, identity_columns, TOXICITY_COLUMN): # no *args or **kargs\n",
    "                self.identity_columns = identity_columns  \n",
    "                self.TOXICITY_COLUMN = TOXICITY_COLUMN\n",
    "        def fit(self, df, y=None):\n",
    "             return self  # nothing else to do\n",
    "        \n",
    "        def transform(self, df):\n",
    "            return convert_dataframe_to_bool(df,self.identity_columns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Function_2(test_df):\n",
    "    identity_columns = [\n",
    "         'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "         'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "    TOXICITY_COLUMN = 'target' \n",
    "    conv_to_bool=ConverScoreToBool(identity_columns,TOXICITY_COLUMN)\n",
    "    test_df=conv_to_bool.transform(test_df) \n",
    "    test_df[\"model_prob\"] = list(pipe_1.predict(test_df.comment_text.astype(str))[:,1])\n",
    "    bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, \"model_prob\", TOXICITY_COLUMN)\n",
    "    evaluation_score  = get_final_metric(bias_metrics_df.fillna(0), calculate_overall_auc(test_df, \"model_prob\"))\n",
    "    \n",
    "    return evaluation_score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result of Function 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Evaluation of model we have to collect following columns:\n",
    "<ol>\n",
    "<li> identity_columns = <b>['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', 'white', 'psychiatric_or_mental_illness']</b>\n",
    "Score for toxicity theshold (between 0-1) with each identity columns </li> \n",
    "\n",
    "<li> comment text</li>\n",
    "<li>target: overall threshold for toxicity in comment</li>\n",
    " </ol>\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Train data to test Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848     0.0  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849     0.0  Thank you!! This would make my life a lot less...   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "1              0.0      0.0              0.0     0.0     0.0    NaN      NaN   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "\n",
       "[2 rows x 45 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embd = pd.read_csv('./data/train.csv', sep=',', quotechar='\"')\n",
    "train_embd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Score of input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24439102564102566"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Function_2(train_embd[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embd.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
